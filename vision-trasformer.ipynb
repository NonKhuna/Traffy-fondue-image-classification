{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/khunanonr/vision-trasformer?scriptVersionId=131556247\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-16T18:00:46.496537Z","iopub.execute_input":"2023-03-16T18:00:46.496847Z","iopub.status.idle":"2023-03-16T18:00:46.530089Z","shell.execute_reply.started":"2023-03-16T18:00:46.496811Z","shell.execute_reply":"2023-03-16T18:00:46.529077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install --q datasets transformers","metadata":{"execution":{"iopub.status.busy":"2023-03-16T18:00:46.532207Z","iopub.execute_input":"2023-03-16T18:00:46.532595Z","iopub.status.idle":"2023-03-16T18:00:59.760918Z","shell.execute_reply.started":"2023-03-16T18:00:46.53256Z","shell.execute_reply":"2023-03-16T18:00:59.759723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport re\nimport os","metadata":{"execution":{"iopub.status.busy":"2023-03-16T18:02:53.634227Z","iopub.execute_input":"2023-03-16T18:02:53.635314Z","iopub.status.idle":"2023-03-16T18:02:53.641244Z","shell.execute_reply.started":"2023-03-16T18:02:53.63526Z","shell.execute_reply":"2023-03-16T18:02:53.640112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nlabel_image = ['canal','electric','flooding','light','road','sanitary','sewer','sidewalk','stray','traffic']\n\nimg_dir = \"/kaggle/input/2110446-data-science-and-data-engineering-2023/TraffyFondue/train/\"\n\nimg_label_map = {}\n\npaths = []\nimage_names = []\nlabel_nums = []\nfor label_num, label in enumerate(label_image):\n    _, _, files = next(os.walk(os.path.join(img_dir,label)))\n    img_label_map[label] = label_num\n    for image_name in files:\n        paths.append(os.path.join(img_dir,label,image_name))\n        label_nums.append(label_num) # [image_path, label_num]\n        image_names.append(image_name)\n        \ndf_dataset = pd.DataFrame({\"path\":paths, \"label\": label_nums, 'filename': image_names})\nprint(df_dataset.shape)\ndf_dataset.tail()","metadata":{"execution":{"iopub.status.busy":"2023-03-16T18:02:54.423363Z","iopub.execute_input":"2023-03-16T18:02:54.424149Z","iopub.status.idle":"2023-03-16T18:02:56.475239Z","shell.execute_reply.started":"2023-03-16T18:02:54.424108Z","shell.execute_reply":"2023-03-16T18:02:56.474078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load file name\n!pip --q install gdown\n!gdown --q 14X-tmO3Ni5a7sKxvVtRaFAfVGJR1rKp8 # file name","metadata":{"execution":{"iopub.status.busy":"2023-03-16T18:02:56.477256Z","iopub.execute_input":"2023-03-16T18:02:56.478119Z","iopub.status.idle":"2023-03-16T18:03:08.450803Z","shell.execute_reply.started":"2023-03-16T18:02:56.478089Z","shell.execute_reply":"2023-03-16T18:03:08.449377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clean_df = pd.read_csv(\"/kaggle/working/filenames (4).csv\")\nnew_df = clean_df.merge(df_dataset, on='filename', how='inner')\nnew_df = new_df.drop(columns=['filename','label'])\nnew_df['label'] = new_df['class'].map(img_label_map)\nnew_df = new_df.drop(columns=['class'])\ndf_dataset = new_df\nnew_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-16T18:03:08.455817Z","iopub.execute_input":"2023-03-16T18:03:08.45688Z","iopub.status.idle":"2023-03-16T18:03:08.511185Z","shell.execute_reply.started":"2023-03-16T18:03:08.456828Z","shell.execute_reply":"2023-03-16T18:03:08.510123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-16T18:03:08.515964Z","iopub.execute_input":"2023-03-16T18:03:08.518187Z","iopub.status.idle":"2023-03-16T18:03:08.532198Z","shell.execute_reply.started":"2023-03-16T18:03:08.518152Z","shell.execute_reply":"2023-03-16T18:03:08.53111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"from transformers import ViTFeatureExtractor\nclass TraffyFondueDataset(Dataset):\n    def __init__(self, \n                 df, \n                label=True):\n        super().__init__()\n        self.df_data = df\n        self.transform = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n        self.label = label\n\n    def __len__(self):\n        return len(self.df_data)\n\n    def __getitem__(self, idx): \n        img = Image.open(self.df_data.loc[idx, 'path']).convert('RGB')\n        x = self.transform(img, return_tensors='pt')['pixel_values']\n#         x = x.view((3,224,224))\n        if self.label :\n            y = self.df_data.loc[idx, 'label']\n            return x,y\n        else : return x, self.df_data.loc[idx, 'path']","metadata":{"execution":{"iopub.status.busy":"2023-03-16T18:03:15.464353Z","iopub.execute_input":"2023-03-16T18:03:15.465061Z","iopub.status.idle":"2023-03-16T18:03:24.607668Z","shell.execute_reply.started":"2023-03-16T18:03:15.465021Z","shell.execute_reply":"2023-03-16T18:03:24.606413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting Data \nfrom sklearn.model_selection import train_test_split\n\nX = df_dataset[['path']]\ny = df_dataset[['label']]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\nX_train, X_val, y_train, y_val = train_test_split( X_train, y_train, test_size=1/9, random_state=42, stratify=y_train)\n\ntrain = X_train.join(y_train).reset_index()\ntest = X_test.join(y_test).reset_index()\nval = X_val.join(y_val).reset_index()\n\ntrain.shape, test.shape, val.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-16T18:03:24.610206Z","iopub.execute_input":"2023-03-16T18:03:24.611015Z","iopub.status.idle":"2023-03-16T18:03:25.175905Z","shell.execute_reply.started":"2023-03-16T18:03:24.610973Z","shell.execute_reply":"2023-03-16T18:03:25.174868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-16T18:10:43.105032Z","iopub.execute_input":"2023-03-16T18:10:43.106059Z","iopub.status.idle":"2023-03-16T18:10:43.115878Z","shell.execute_reply.started":"2023-03-16T18:10:43.106001Z","shell.execute_reply":"2023-03-16T18:10:43.114631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_cat = train['label'].value_counts() \ntarget_amount = 1100\npaths = []\nlabels = []\nfor i in range(10) :\n    data = train[train['label'] == i].sample(n=target_amount, replace=count_cat[i] < target_amount, random_state=25)\n    paths += list(data['path'])\n    labels += list(data['label'])\nnew_train = pd.DataFrame({'path':paths, \"label\":labels})\nnew_train['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-03-16T18:11:24.772689Z","iopub.execute_input":"2023-03-16T18:11:24.773188Z","iopub.status.idle":"2023-03-16T18:11:24.837298Z","shell.execute_reply.started":"2023-03-16T18:11:24.773144Z","shell.execute_reply":"2023-03-16T18:11:24.836238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\n\ntrainset = TraffyFondueDataset(new_train)\nvalset = TraffyFondueDataset(val)\ntestset = TraffyFondueDataset(test)\n\ntrain_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = torch.utils.data.DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-16T18:11:25.127584Z","iopub.execute_input":"2023-03-16T18:11:25.12819Z","iopub.status.idle":"2023-03-16T18:11:25.721816Z","shell.execute_reply.started":"2023-03-16T18:11:25.128142Z","shell.execute_reply":"2023-03-16T18:11:25.720791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# functions to show an image\ndef imshow(img):\n    img = img*torch.tensor([0.5, 0.5, 0.5]).mean() + torch.tensor([0.5, 0.5, 0.5]).mean()     # unnormalize\n    npimg = img.numpy()\n    plt.figure(figsize=(16,16))\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# get some random training images\ndataiter = iter(train_loader)\nimages, labels = next(dataiter)\n# print(images.shape)\nimages = images.view((BATCH_SIZE ,3,224,224))\n\n# show images\nnrow = 9\nimshow(torchvision.utils.make_grid(images, nrow = nrow))\nprint(labels)","metadata":{"execution":{"iopub.status.busy":"2023-03-16T18:11:27.870178Z","iopub.execute_input":"2023-03-16T18:11:27.871242Z","iopub.status.idle":"2023-03-16T18:11:35.882267Z","shell.execute_reply.started":"2023-03-16T18:11:27.871183Z","shell.execute_reply":"2023-03-16T18:11:35.880898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"from transformers import ViTModel\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ViTForImageClassification(nn.Module):\n    def __init__(self, num_labels=3):\n        super(ViTForImageClassification, self).__init__()\n        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n        self.num_labels = num_labels\n\n    def forward(self, pixel_values, labels):\n        outputs = self.vit(pixel_values=pixel_values)\n        output = self.dropout(outputs.last_hidden_state[:,0])\n        logits = self.classifier(output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n        if loss is not None:\n            return logits, loss.item()\n        else:\n            return logits, None","metadata":{"execution":{"iopub.status.busy":"2023-03-16T18:12:24.710573Z","iopub.execute_input":"2023-03-16T18:12:24.711461Z","iopub.status.idle":"2023-03-16T18:12:25.440952Z","shell.execute_reply.started":"2023-03-16T18:12:24.711419Z","shell.execute_reply":"2023-03-16T18:12:25.439713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import classification_report\nimport torch\n\nnum_classes = len(img_label_map)\n\nLEARNING_RATE = 2e-5\n\nmodel = ViTForImageClassification(num_classes)    \n\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\nloss_func = nn.CrossEntropyLoss()\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \nif torch.cuda.is_available():\n    model.cuda() ","metadata":{"execution":{"iopub.status.busy":"2023-03-16T18:12:27.810331Z","iopub.execute_input":"2023-03-16T18:12:27.811285Z","iopub.status.idle":"2023-03-16T18:12:36.327562Z","shell.execute_reply.started":"2023-03-16T18:12:27.81123Z","shell.execute_reply":"2023-03-16T18:12:36.32648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"import torch.utils.data as data\nfrom torch.autograd import Variable\n\n\nEPOCHS = 5\n\n\nhistory_train = {'loss':np.zeros(EPOCHS), 'acc':np.zeros(EPOCHS), 'f1-score':np.zeros(EPOCHS)}\nhistory_val = {'loss':np.zeros(EPOCHS), 'acc':np.zeros(EPOCHS), 'f1-score':np.zeros(EPOCHS)}\nmin_val_loss = 1e10\nPATH = './best_model.pth'\n\n# Train the model\nfor epoch in range(EPOCHS): \n    \n    print(f'epoch {epoch + 1} \\nTraining ...')\n    model.train()\n    y_predict = list()\n    y_labels = list()\n    training_loss = 0.0\n    n = 0\n    for step, (x, y) in enumerate(tqdm(train_loader)):\n   \n        # Apply feature extractor, stack back into 1 tensor and then convert to tensor\n        x = x.view((-1,3,224,224))\n        x, y  = x.to(device), y.to(device)\n        b_x = Variable(x)   # batch x (image)\n        b_y = Variable(y)   # batch y (target)\n        # Feed through model\n        output, loss = model(b_x, None)\n        # Calculate loss\n        if loss is None: \n            loss = loss_func(output, b_y)   \n            optimizer.zero_grad()           \n            loss.backward()                 \n            optimizer.step()\n        \n        training_loss += loss.item()\n        n+=1\n        y_labels += list(y.cpu().numpy())\n        y_predict += list(output.argmax(dim=1).cpu().numpy())\n\n    # print statistics\n    report = classification_report(y_labels, y_predict, digits = 4, output_dict = True)\n    acc = report[\"accuracy\"]\n    f1 = report[\"weighted avg\"][\"f1-score\"]\n    support = report[\"weighted avg\"][\"support\"]\n    training_loss /= n\n    print(f\"training loss: {training_loss:.4}, acc: {acc*100:.4}%, f1-score: {f1*100:.4}%, support: {support}\" )\n    history_train['loss'][epoch] = training_loss\n    history_train['acc'][epoch] = acc\n    history_train['f1-score'][epoch] = f1\n    \n    print('validating ...')\n    model.eval()\n    optimizer.zero_grad()\n    \n    y_predict = list()\n    y_labels = list()\n    validation_loss = 0.0\n    n = 0\n    with torch.no_grad():\n        for data in tqdm(val_loader):\n            inputs, labels = data\n            inputs = inputs.view((-1,3,224,224))\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs, loss = model(inputs, labels)\n            \n            validation_loss += loss\n\n            y_labels += list(labels.cpu().numpy())\n            y_predict += list(outputs.argmax(dim=1).cpu().numpy())\n            n+=1\n            \n    # print statistics\n    report = classification_report(y_labels, y_predict, digits = 4, output_dict = True)\n    acc = report[\"accuracy\"]\n    f1 = report[\"weighted avg\"][\"f1-score\"]\n    support = report[\"weighted avg\"][\"support\"]\n    validation_loss /= n\n    print(f\"validation loss: {validation_loss:.4}, acc: {acc*100:.4}%, f1-score: {f1*100:.4}%, support: {support}\" )\n    history_val['loss'][epoch] = validation_loss\n    history_val['acc'][epoch] = acc\n    history_val['f1-score'][epoch] = f1\n    \n    #save min validation loss\n    if validation_loss < min_val_loss:\n        torch.save(model.state_dict(), PATH)\n        min_val_loss = validation_loss   \n        print(\"Save best model\")\n    \nprint('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2023-03-16T14:52:00.465172Z","iopub.execute_input":"2023-03-16T14:52:00.465948Z","iopub.status.idle":"2023-03-16T15:50:03.444367Z","shell.execute_reply.started":"2023-03-16T14:52:00.465908Z","shell.execute_reply":"2023-03-16T15:50:03.439134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate model","metadata":{}},{"cell_type":"code","source":"# !gdown 1eJ-78n7NYvMqc1u9-E2Evo0JraZsZ0VF","metadata":{"execution":{"iopub.status.busy":"2023-03-16T15:50:03.446144Z","iopub.execute_input":"2023-03-16T15:50:03.446848Z","iopub.status.idle":"2023-03-16T15:50:03.453552Z","shell.execute_reply.started":"2023-03-16T15:50:03.446803Z","shell.execute_reply":"2023-03-16T15:50:03.452404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = './best_model.pth'\nbest_model = ViTForImageClassification(num_classes)  \n\nbest_model = best_model.to(device)\nbest_model.load_state_dict(torch.load(PATH))","metadata":{"execution":{"iopub.status.busy":"2023-03-16T15:50:03.456624Z","iopub.execute_input":"2023-03-16T15:50:03.457464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n\nprint('testing ...')\ny_predict = list()\ny_labels = list()\n\nwith torch.no_grad():\n    for data in tqdm(test_loader) :\n        best_model.eval()\n        inputs, target = data\n        inputs = inputs.view((-1,3,224,224))\n        # Send to appropriate computing device\n        inputs = inputs.to(device)\n        target = target.to(device)\n        \n        \n\n        # Generate prediction\n        prediction, loss = best_model(inputs, target)\n\n        # Predicted class value using argmax\n        y_predict += list(prediction.argmax(dim=1).cpu().numpy())\n        y_labels += list(target.cpu().numpy())\nreport = classification_report(y_labels, y_predict, digits = 4)\nprint(report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Export output","metadata":{}},{"cell_type":"code","source":"id_range = []\ntest_dir = \"/kaggle/input/2110446-data-science-and-data-engineering-2023/TraffyFondue/test/\"\npaths = []\nlabel_nums = []\nlabel_num = 0\nfilename = [os.path.join(test_dir, x) for x in sorted(os.listdir(test_dir))]\ntest_submit_df = pd.DataFrame({\"path\":filename})\ntest_submit_dataset = TraffyFondueDataset(test_submit_df, label=False)\n\ntest_submit_loader = DataLoader(test_submit_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nIDs = []\ny_out = []\n\nwith torch.no_grad():\n    for data in tqdm(test_submit_loader):\n        best_model.eval()\n        inputs, path =  data\n        inputs = inputs.view((-1,3,224,224))\n        # Send to appropriate computing device\n        inputs = inputs.to(device)\n\n        # Generate prediction\n        prediction, loss = best_model(inputs, None)\n        IDs += list([re.sub(\"(.*test|\\.jpg)\", \"\", x) for x in path])\n        y_out += list(prediction.argmax(dim=1).cpu().numpy())\n\nresult = pd.DataFrame({\"class\":y_out, \"ID\": IDs})\nresult = result.set_index('ID')\nresult.to_csv(\"submiting_ds.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}